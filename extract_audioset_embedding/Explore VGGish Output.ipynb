{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile\n",
    "import librosa\n",
    "\n",
    "import tensorflow as tf\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "slim = tf.contrib.slim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path, target_fs=None):\n",
    "    (audio, fs) = soundfile.read(path)\n",
    "\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "        \n",
    "    if target_fs is not None and fs != target_fs:\n",
    "        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)\n",
    "        fs = target_fs\n",
    "        \n",
    "    return audio, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../vggish_model.ckpt\n",
      "Audio length: 75860\n",
      "Log mel shape: (9, 96, 64)\n",
      "Embedding feature shape: (9, 128)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extract log mel spectrogram features. \n",
    "\"\"\"\n",
    "\n",
    "# Arguments & parameters\n",
    "mel_bins = vggish_params.NUM_BANDS\n",
    "sample_rate = vggish_params.SAMPLE_RATE\n",
    "input_len = vggish_params.NUM_FRAMES\n",
    "embedding_size = vggish_params.EMBEDDING_SIZE\n",
    "\n",
    "'''You may modify the EXAMPLE_HOP_SECONDS in vggish_params.py to change the \n",
    "hop size. '''\n",
    "\n",
    "# Paths\n",
    "audio_path = '../appendixes/01.wav'#'../appendixes/chunk-01.wav'#\n",
    "checkpoint_path = os.path.join('../vggish_model.ckpt')\n",
    "pcm_params_path = os.path.join('../vggish_pca_params.npz')\n",
    "\n",
    "if not os.path.isfile(checkpoint_path):\n",
    "    raise Exception('Please download vggish_model.ckpt from '\n",
    "        'https://storage.googleapis.com/audioset/vggish_model.ckpt '\n",
    "        'and put it in the root of this codebase. ')\n",
    "\n",
    "if not os.path.isfile(pcm_params_path):\n",
    "    raise Exception('Please download pcm_params_path from '\n",
    "    'https://storage.googleapis.com/audioset/vggish_pca_params.npz '\n",
    "    'and put it in the root of this codebase. ')\n",
    "\n",
    "# Load model\n",
    "sess = tf.Session()\n",
    "\n",
    "vggish_slim.define_vggish_slim(training=False)\n",
    "vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "pproc = vggish_postprocess.Postprocessor(pcm_params_path)\n",
    "\n",
    "# Read audio\n",
    "(audio, _) = read_audio(audio_path, target_fs=sample_rate)\n",
    "\n",
    "# Extract log mel feature\n",
    "logmel = vggish_input.waveform_to_examples(audio, sample_rate)\n",
    "\n",
    "# Extract embedding feature\n",
    "[embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: logmel})\n",
    "\n",
    "# PCA\n",
    "postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "\n",
    "print('Audio length: {}'.format(len(audio)))\n",
    "print('Log mel shape: {}'.format(logmel.shape))\n",
    "print('Embedding feature shape: {}'.format(postprocessed_batch.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postprocessed_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(postprocessed_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_batch.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audioset_tensorflow",
   "language": "python",
   "name": "audioset_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
